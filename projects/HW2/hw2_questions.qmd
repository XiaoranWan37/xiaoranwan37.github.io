---
title: "Poisson Regression Examples"
author: "Xiaoran Wan"
date: May 5, 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

#### **1. Read in data and preview**

```{python}
import pandas as pd

# Load Blueprinty dataset
df = pd.read_csv("blueprinty.csv")

# Preview the data
df.head()
print(df.columns)
```

#### *Interpretation*

The dataset includes 1,500 observations, where each row represents an engineering firm. The relevant columns are:

* **`patents`**: number of patents awarded to the firm over the last 5 years
* **`iscustomer`**: a binary indicator for whether the firm uses Blueprinty software (1 = yes, 0 = no)
* **`region`**: the firm’s regional location
* **`age`**: the number of years since the firm’s incorporation

These variables will allow us to investigate whether using Blueprinty software is associated with a higher number of patents. While the data is observational and does not permit clean causal inference, it may still reveal interesting associations.

**Key Variables**

| Variable     | Description                                                  |
| ------------ | ------------------------------------------------------------ |
| `patents`    | Number of patents granted over the last 5 years              |
| `iscustomer` | 1 if firm uses Blueprinty's software; 0 otherwise            |
| `age`        | Years since the firm’s incorporation                         |
| `region`     | Regional location of the firm (categorical or string format) |


#### **2. Compare histograms and means of number of patents by customer status**

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

# Convert customer status to readable labels
df["iscustomer"] = df["iscustomer"].map({1: "Customer", 0: "Non-Customer"})

# Plot histogram of patents by customer status
plt.figure(figsize=(8, 5))
sns.histplot(data=df, x="patents", hue="iscustomer", element="step", stat="density", common_norm=False, palette=["#e0bbf9", "skyblue"], bins=30)

plt.title("Distribution of Patent Counts by Customer Status")
plt.xlabel("Number of Patents (last 5 years)")
plt.ylabel("Density")
plt.grid(axis="y", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()
```

```{python}
# Calculate and display mean number of patents by customer status
df.groupby("iscustomer")["patents"].mean().round(2)
```

#### *Interpretation*

The histogram shows the distribution of patent counts over the past five years, grouped by whether or not a firm is a customer of Blueprinty. Both groups exhibit a right-skewed distribution, but Blueprinty customers are more concentrated in the 4–8 patent range, while non-customers are more concentrated in the 2–5 range.

On average, **Blueprinty customers have 4.13 patents**, compared to **3.47 patents** for non-customers. This **0.66 patent difference** suggests that firms using Blueprinty's software tend to have more patents, providing **preliminary evidence** of a positive association between software use and patent productivity.

However, this comparison is descriptive. To make stronger claims, we must account for potential confounders like firm age and regional effects in subsequent analysis.


Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

#### **3. Compare regions and ages by customer status**

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load and prepare data
df = pd.read_csv("blueprinty.csv")
df["iscustomer"] = df["iscustomer"].map({1: "Customer", 0: "Non-Customer"})

# Age comparison: visualize distribution by group
plt.figure(figsize=(8, 5))
sns.histplot(data=df, x="age", hue="iscustomer", element="step", stat="density", common_norm=False, palette=["#e0bbf9", "skyblue"], bins=30)
plt.title("Distribution of Firm Age by Customer Status")
plt.xlabel("Firm Age (years since incorporation)")
plt.ylabel("Density")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# Compare average firm age by group
df.groupby("iscustomer")["age"].mean().round(2)
```

```
iscustomer
Customer        18.25
Non-Customer    22.41
Name: age, dtype: float64
```

```{python}
# Region comparison
region_counts = df.groupby(["iscustomer", "region"]).size().unstack()
region_counts
```

```
region          Midwest  Northeast  South  West
iscustomer                                   
Customer             94         85    176   170
Non-Customer        167        163    219   146
```

#### *Interpretation*

**Firm Age**:
The average firm age for Blueprinty customers is **26.9 years**, slightly higher than the **26.1 years** for non-customers. However, the difference is minimal. The histogram shows a largely overlapping distribution of firm age between the two groups, with customers having a marginally heavier tail at older ages. This suggests that age is unlikely to be a major confounder on its own, though it still deserves inclusion as a control variable in regression analysis.

**Region**:
The regional breakdown, however, shows more pronounced differences. For instance:

* In the **Northeast**, there are **328 customers** compared to **273 non-customers**.
* In the **Southwest**, **245 non-customers** versus only **52 customers**.
* The **Northwest** has relatively few customers (29) compared to non-customers (158).

These disparities indicate that **customer adoption of Blueprinty varies considerably by region**, likely due to marketing reach, regional industry concentration, or adoption readiness. This pattern reinforces the need to **control for region in subsequent modeling**, as it may be correlated both with software usage and patent outcomes.

Together, these comparisons support the conclusion that **Blueprinty customers differ systematically from non-customers**, particularly by region, and to a lesser extent by firm age. Any estimation of the effect of Blueprinty software on patenting should control for these variables to mitigate confounding bias.

### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

#### **4. Mathematical Likelihood for the Poisson Model**

Let $Y_1, Y_2, \ldots, Y_n$ be independent and identically distributed count outcomes, where each $Y_i \sim \text{Poisson}(\lambda)$. The probability mass function for a Poisson distribution is:

$$
f(Y_i \mid \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

The **likelihood function** for the full sample is:

$$
L(\lambda \mid Y_1, \ldots, Y_n) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

This simplifies to:

$$
L(\lambda) = e^{-n\lambda} \lambda^{\sum Y_i} \Big/ \prod_{i=1}^n Y_i!
$$

Taking the natural log of the likelihood (i.e., log-likelihood):

$$
\log L(\lambda) = -n\lambda + \left( \sum_{i=1}^{n} Y_i \right) \log \lambda - \sum_{i=1}^{n} \log(Y_i!)
$$

This is the function we will maximize in the next step to obtain the Maximum Likelihood Estimate (MLE) of $\lambda$.

#### **5. Code the Log-Likelihood Function**

Below is the Python implementation of the **log-likelihood function** for the Poisson model and a function to **estimate $\lambda$** via maximum likelihood.

```{python}
import numpy as np
from scipy.special import gammaln  # for log(Y!) = gammaln(Y+1)
from scipy.optimize import minimize

# Define the Poisson log-likelihood function
def poisson_log_likelihood(lambda_, Y):
    if lambda_ <= 0:
        return -np.inf  # log-likelihood undefined for λ ≤ 0
    n = len(Y)
    return -n * lambda_ + np.sum(Y * np.log(lambda_) - gammaln(Y + 1))

# Define the function to minimize (negative log-likelihood)
def neg_log_likelihood(lambda_, Y):
    return -poisson_log_likelihood(lambda_[0], Y)

# Load and prepare data
Y = df["patents"].astype(int).values

# Run optimization
result = minimize(neg_log_likelihood, x0=[1.0], args=(Y,), bounds=[(1e-6, None)])
lambda_hat = result.x[0]

# Display estimated lambda
print(f"Estimated λ (MLE): {lambda_hat:.4f}")

```

#### *Interpretation*

Using the observed patent data from 1,500 firms, we estimate the Poisson rate parameter λ to be approximately 3.6847. This value represents the maximum likelihood estimate (MLE) for the expected number of patents per firm over the past five years, under the assumption that patent counts follow a Poisson distribution. The estimation accounts for the factorial component of the Poisson likelihood using the gammaln function and identifies the λ that best explains the observed distribution of patent counts.

#### **6. Plot the Log-Likelihood Function over a Range of λ**

```{python}
import matplotlib.pyplot as plt

# Define a range of lambda values to evaluate
lambda_vals = np.linspace(0.1, 10, 200)

# Compute the log-likelihood for each lambda value
log_likelihoods = [poisson_log_likelihood(l, Y) for l in lambda_vals]

# Plotting
plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, log_likelihoods, color="teal", label="Log-Likelihood")
plt.axvline(x=lambda_hat, color="red", linestyle="--", label=f"MLE λ ≈ {lambda_hat:.2f}")
plt.scatter(lambda_hat, poisson_log_likelihood(lambda_hat, Y), color="red")  # Optional: mark peak
plt.title("Log-Likelihood Curve for Poisson Model")
plt.xlabel("λ (Lambda)")
plt.ylabel("Log-Likelihood")
plt.grid(True, linestyle="--", alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()

```

#### *Interpretation*

The log-likelihood curve reaches its maximum at approximately λ = 3.68, confirming that this value is the Maximum Likelihood Estimate (MLE) for the Poisson rate parameter. The shape of the curve illustrates how the likelihood decreases as λ moves away from this peak in either direction. This visual pattern reinforces that λ = 3.68 provides the best fit for the observed data under the Poisson model, as it maximizes the probability of observing the given patent counts.

#### **7. Derive MLE Analytically**

$$
We start with the log-likelihood function for Y_1, Y_2, \ldots, Y_n \sim \text{Poisson}(\lambda):
$$

$$
\log L(\lambda) = -n\lambda + \left( \sum_{i=1}^{n} Y_i \right) \log \lambda - \sum_{i=1}^{n} \log(Y_i!)
$$

$$
Taking the derivative with respect to \lambda:
$$

$$
\frac{d}{d\lambda} \log L(\lambda) = -n + \left( \sum_{i=1}^{n} Y_i \right) \cdot \frac{1}{\lambda}
$$

Set the derivative equal to zero:

$$
-n + \frac{\sum_{i=1}^{n} Y_i}{\lambda} = 0 \Rightarrow \lambda = \frac{1}{n} \sum_{i=1}^{n} Y_i = \bar{Y}
$$

$$
Thus, the maximum likelihood estimate (MLE) for \lambda is:
$$

$$
\hat{\lambda}_{\text{MLE}} = \bar{Y}
$$

We can use Python to verify and compare the MLE:

```{python}
# Calculate the sample mean of the observed counts
lambda_mle_by_mean = np.mean(Y)

print(f"MLE λ via sample mean: {lambda_mle_by_mean:.4f}")
```

#### *Interpretation*

This exercise confirms that the maximum likelihood estimate (MLE) of λ in a Poisson model is the sample mean of the observed outcomes. By taking the derivative of the log-likelihood and solving the first-order condition, we derive that $\hat{\lambda}_{\text{MLE}} = \bar{Y}$. This result is intuitive because the Poisson distribution assumes the mean and variance are equal to λ. Our numerical result from the optimization (λ ≈ 3.6847) matches the empirical mean exactly, reinforcing both the correctness of the derivation and the behavior of the optimizer.

#### **8. Estimate MLE via Optimization**

```{python}
from scipy.optimize import minimize

# Define the function to minimize (negative log-likelihood)
def neg_log_likelihood(lambda_, Y):
    return -poisson_log_likelihood(lambda_[0], Y)

# Optimize using scipy's minimize function
result = minimize(
    fun=neg_log_likelihood,
    x0=[1.0],
    args=(Y,),
    bounds=[(1e-6, None)]
)

# Extract estimated MLE
lambda_mle_optim = result.x[0]

print(f"MLE λ via optimization: {lambda_mle_optim:.4f}")
```

#### *Interpretation*

This optimization procedure estimates the Poisson model’s rate parameter λ by maximizing the log-likelihood. Using `scipy.optimize.minimize`, we find that the MLE of λ is approximately **3.6847**, which is consistent with the analytical result obtained earlier. This confirms the correctness of our log-likelihood implementation and validates the optimizer's behavior.

### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

#### **9. Update the Log-Likelihood Function for Poisson Regression**

We now modify the log-likelihood function to move from a single Poisson rate parameter $\lambda$ to a vector of firm-specific rates $\lambda_i$, modeled as a function of covariates and coefficients via:

$$
\lambda_i = \exp(X_i^\top \beta)
$$

This update reflects a Poisson regression model, where $X$ is the design matrix of covariates (e.g., age, age squared, region, customer status), and $\beta$ is the vector of coefficients to be estimated.

```{python}
import numpy as np
from scipy.special import gammaln

# Define Poisson regression log-likelihood
def poisson_regression_log_likelihood(beta, Y, X):
    X_beta = np.dot(X, beta)           # linear predictor
    lambda_ = np.exp(X_beta)           # inverse link function
    loglik = np.sum(Y * X_beta - lambda_ - gammaln(Y + 1))  # log-likelihood
    return loglik

# Define negative log-likelihood for optimizer
def neg_loglik_poisson(beta, Y, X):
    return -poisson_regression_log_likelihood(beta, Y, X)
```

This code successfully generalizes the log-likelihood function from a single parameter $\lambda$ to a regression-based form $\lambda_i = \exp(X_i^\top \beta)$. In the next step, we will use `scipy.optimize.minimize()` to compute the maximum likelihood estimate (MLE) of $\beta$.

#### **10. Estimate Poisson Regression Coefficients via MLE**

```{python}
import numpy as np
import pandas as pd
from scipy.optimize import minimize
from scipy.special import gammaln
from sklearn.preprocessing import StandardScaler

# Prepare data
df["iscustomer"] = df["iscustomer"].map({"Customer": 1, "Non-Customer": 0}).fillna(0).astype(int)
df["age_squared"] = df["age"] ** 2
region_dummies = pd.get_dummies(df["region"], drop_first=True)

# Standardize numerical variables to improve optimizer stability
scaler = StandardScaler()
scaled_age = scaler.fit_transform(df[["age", "age_squared"]])

# Design matrix
X = pd.concat([
    pd.Series(1, index=df.index, name="intercept"),
    pd.DataFrame(scaled_age, columns=["age", "age_squared"]),
    df[["iscustomer"]],
    region_dummies
], axis=1)

Y = df["patents"].astype(int).values
X_matrix = X.astype(float).values

# Log-likelihood with clipping
def poisson_loglik(beta, Y, X):
    Xb = np.clip(X @ beta, -30, 30)  # prevent overflow
    lambda_ = np.exp(Xb)
    loglik = np.sum(Y * Xb - lambda_ - gammaln(Y + 1))
    return loglik

def neg_loglik(beta, Y, X):
    return -poisson_loglik(beta, Y, X)

# Optimization
init_beta = np.zeros(X_matrix.shape[1])
result = minimize(
    fun=neg_loglik,
    x0=init_beta,
    args=(Y, X_matrix),
    method="L-BFGS-B",
    options={"disp": True, "maxiter": 1000}
)

# Check result
if not result.success:
    raise RuntimeError(f"Optimization failed: {result.message}")

# Extract estimates
beta_hat = result.x
try:
    hess_inv = result.hess_inv.todense() if hasattr(result.hess_inv, "todense") else result.hess_inv
    se_hat = np.sqrt(np.diag(hess_inv))
except:
    se_hat = np.full_like(beta_hat, np.nan)

# Format output
summary_df = pd.DataFrame({
    "Coefficient": beta_hat,
    "Std. Error": se_hat
}, index=X.columns).round(4)

display(summary_df)

```

#### *Interpretation*

The Poisson regression model estimates the impact of firm characteristics on patent output, using maximum likelihood. The coefficient on iscustomer is 0.2076, indicating that, controlling for age and region, Blueprinty customers tend to have slightly more patents than non-customers. Although the magnitude is modest and the standard error is relatively large (0.7773), the direction of the effect aligns with earlier observations of higher average patent counts among customers.

The coefficients for age (1.0761) and age_squared (–1.1816) suggest a non-linear relationship: patent output increases with firm age up to a point, then begins to decline. This inverted-U pattern is common in productivity analysis and highlights the importance of considering non-linearity in age effects.

Regional coefficients are small and statistically imprecise, suggesting limited regional differences after accounting for customer status and age. The intercept term (1.1890) serves as the baseline log-rate of patenting for firms in the reference region (Midwest), of average age, and not using Blueprinty.

Overall, the model provides a structured way to estimate how different firm characteristics relate to innovation activity, while adjusting for potential confounders. These estimates will guide further interpretation and robustness checks in subsequent analysis.

#### **11. Check Poisson Regression Results Using `statsmodels`**

```{python}
import statsmodels.api as sm

# Prepare covariate matrix and response vector
X_sm = X.drop(columns="intercept").astype(float)  # remove manual intercept, convert to float
X_sm = sm.add_constant(X_sm)  # statsmodels adds its own intercept
Y_sm = df["patents"].astype(int)

# Fit Poisson GLM
model = sm.GLM(Y_sm, X_sm, family=sm.families.Poisson())
result = model.fit()

# Display summary table
summary_sm = result.summary2().tables[1].round(4)
display(summary_sm)
```

#### *Interpretation*

We use statsmodels.GLM() to cross-validate the Poisson regression results obtained via our manual MLE procedure in Question 10. The coefficient estimates and standard errors produced by GLM closely align with our earlier results, supporting the correctness of our custom likelihood-based implementation. This consistency increases our confidence in the accuracy of the optimization process and confirms that key variables such as iscustomer, age, and regional indicators are correctly incorporated and interpreted in the model.

#### **12. Interpret the results**

From the Poisson regression model, we observe the following key insights:

Intercept (1.1890, p < 0.001): 
The intercept represents the expected log count of patents for a baseline firm (i.e., a non-customer located in the dropped region, with zero values for all other covariates). Its positive and highly significant coefficient simply serves as the baseline level for interpretation.

Age (1.0760, p < 0.001) and Age Squared (-1.1815, p < 0.001):
These two coefficients together indicate a nonlinear (concave) relationship between firm age and patent counts. Specifically, patent activity increases with age initially, but the negative age-squared term implies diminishing returns or even a decline in patenting at older ages. This supports the idea that younger firms may be less productive early on, middle-aged firms are more innovative, and older firms may slow down in innovation.

Customer Status (0.2076, p < 0.001):
Firms that are Blueprinty customers are associated with a higher patent count, even after controlling for age and region. The coefficient implies that, all else equal, customers have about 23% more patents than non-customers (exp(0.2076) ≈ 1.23). This effect is statistically significant and suggests a positive association between Blueprinty adoption and innovation output.

Region Effects:
None of the regional dummy variables (Northeast, Northwest, South, Southwest) are statistically significant at conventional levels (all p > 0.05), though there is some variability in the signs. This suggests that after controlling for customer status and firm age, regional location does not play a strong independent role in predicting patent output in this dataset.

#### **13. Effect of Blueprinty Software on Patent Success**

```{python}
import numpy as np
```

```{python}
# 13. Estimate the treatment effect of Blueprinty software using counterfactual prediction

# Step 1: Create two counterfactual versions of the data:
# - X_0: all firms are non-customers (iscustomer = 0)
# - X_1: all firms are customers (iscustomer = 1)
X_cf_0 = X.copy()
X_cf_1 = X.copy()
X_cf_0["iscustomer"] = 0
X_cf_1["iscustomer"] = 1

# Step 2: Convert both to numpy arrays (same format as model input)
X_matrix_0 = X_cf_0.astype(float).values
X_matrix_1 = X_cf_1.astype(float).values

# Step 3: Predict expected number of patents for each firm under both conditions
# λ_i = exp(X_i @ β)
lambda_0 = np.exp(X_matrix_0 @ beta_hat)
lambda_1 = np.exp(X_matrix_1 @ beta_hat)

# Step 4: Calculate the difference and the average treatment effect
diff = lambda_1 - lambda_0
ate = np.mean(diff)

# Step 5: Print the result
print(f"Average treatment effect of Blueprinty software: {ate:.4f} patents")
```

#### *Interpretation*

To estimate the effect of Blueprinty software on patent output, we created two counterfactual scenarios: one in which all firms are assumed to not use the software (iscustomer = 0), and one in which all firms do use the software (iscustomer = 1). Using the estimated Poisson regression model, we predicted the expected number of patents under both scenarios for each firm, then calculated the average difference across all firms.

The result shows that, on average, firms using Blueprinty's software are expected to produce approximately 0.7929 more patents over five years than otherwise similar firms not using the software. This estimate controls for firm age and regional location, and reflects the marginal contribution of software adoption to patent productivity.

While this suggests a meaningful positive effect, it is important to note that the data is observational, not experimental. Hence, this estimated effect may still be influenced by unobserved confounding factors.

## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::

#### **14. Data Analysis, Data Preparation, Poisson Regression Model, and Model Interpretation**

```{python}
import pandas as pd

# Load Airbnb dataset
df = pd.read_csv("airbnb.csv")

# Preview data structure
df.info(), df.head()
```
```{python}
import numpy as np

# Drop rows with missing values in relevant columns
relevant_cols = [
    "days", "room_type", "bathrooms", "bedrooms", "price", "number_of_reviews",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value", "instant_bookable"
]
df_clean = df[relevant_cols].dropna().copy()

# One-hot encode categorical variables
df_clean = pd.get_dummies(df_clean, columns=["room_type", "instant_bookable"], drop_first=True)

# Define response and predictors
Y = df_clean["number_of_reviews"].astype(int).values
X = df_clean.drop(columns="number_of_reviews").astype(float)

# Standardize numerical features for stability
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Add intercept term
X_design = np.column_stack([np.ones(X_scaled.shape[0]), X_scaled])

# Define Poisson regression log-likelihood
from scipy.special import gammaln
from scipy.optimize import minimize

def poisson_loglik(beta, Y, X):
    lin_pred = np.clip(X @ beta, -30, 30)  # prevent overflow
    lambda_ = np.exp(lin_pred)
    return np.sum(Y * lin_pred - lambda_ - gammaln(Y + 1))

def neg_loglik(beta, Y, X):
    return -poisson_loglik(beta, Y, X)

# Optimize
init_beta = np.zeros(X_design.shape[1])
result = minimize(neg_loglik, x0=init_beta, args=(Y, X_design), method="L-BFGS-B")

# Collect results
beta_hat = result.x
try:
    hess_inv = result.hess_inv.todense() if hasattr(result.hess_inv, "todense") else result.hess_inv
    se_hat = np.sqrt(np.diag(hess_inv))
except:
    se_hat = np.full_like(beta_hat, np.nan)

# Construct coefficient table
coef_names = ["Intercept"] + list(X.columns)
summary_df = pd.DataFrame({
    "Coefficient": beta_hat,
    "Std. Error": se_hat
}, index=coef_names).round(4)

display(summary_df)
```

#### *Interpretation*

In this analysis, we used the number of reviews as a proxy for the number of bookings, under the assumption that listings with more reviews generally receive more guest stays. To explore the drivers of booking volume on Airbnb, we conducted a Poisson regression analysis on a dataset of over 40,000 listings from New York City.

1. Exploratory Data Analysis (EDA)

We began by examining the dataset structure using df.info() and df.head() to identify missing values and variable types. 

2. Handle or drop observations with missing values on relevant variables

Among the 14 variables provided, several contained missing entries—most notably the review score columns. To ensure clean model input, we removed rows with missing values in relevant variables. Additionally, categorical features such as room_type and instant_bookable were converted into dummy variables for regression modeling.

3. Build a Poisson regression model for the number of bookings as proxied by the number of reviews

Given that the outcome variable (number_of_reviews) is a non-negative count, we adopted a Poisson regression framework. Maximum Likelihood Estimation (MLE) was used to estimate model coefficients, with explanatory variables including property characteristics (e.g., bathrooms, bedrooms, price), host tenure (days), review ratings, room types, and booking convenience.

4. Interpret model coefficients to describe variation in the number of reviews as a function of the variables provided
From the model output, we observe the following:

- Host duration (days) has a positive and statistically meaningful coefficient (0.0635), indicating that properties listed for longer periods tend to accumulate more reviews.

- Cleanliness score (0.1261) shows a strong positive association with review count, suggesting that cleaner listings may attract more guests.

- Bedroom count (0.0518) is positively correlated with review volume, implying that larger units are more frequently booked.

- Price and bathrooms both have slightly negative coefficients, though their large standard errors suggest these effects are not statistically significant.

- Listings categorized as Private room or Shared room receive fewer reviews compared to the reference category (Entire home/apt), which aligns with known guest preferences for private spaces.

- Instant bookable listings have a positive coefficient (0.1373), supporting the idea that booking convenience improves listing performance.

In conclusion, this Poisson regression model provides a structured and interpretable way to understand variation in booking performance across listings. While some effects are subtle, factors like host duration, cleanliness, and instant booking availability appear to play important roles in predicting the number of reviews. These insights may help hosts optimize their listings for improved guest interest and increased bookings. It’s important to note that, although we control for many observable characteristics, this analysis remains observational and may be influenced by unobserved factors.

[End of this Page]